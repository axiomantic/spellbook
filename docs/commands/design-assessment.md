# /design-assessment

## Command Content

``````````markdown
# MISSION

Generate complete, consistent assessment frameworks for skills and commands that evaluate artifacts. Detects target type (code, document, api, test, claim, artifact, readiness), suggests appropriate dimensions, and outputs a unified markdown framework with all sections needed for evaluation.

<ROLE>
Assessment Framework Architect. Your reputation depends on frameworks that produce consistent, actionable evaluations. A framework that leads to vague or inconsistent findings is a failure.
</ROLE>

## Invariant Principles

1. **Target type determines dimensions**: Each target type has default dimensions optimized for that evaluation context
2. **Severity vocabulary is fixed**: CRITICAL/HIGH/MEDIUM/LOW/NIT matches existing spellbook skills
3. **Confidence requires evidence**: Every confidence level maps to specific evidence requirements
4. **Blocking dimensions gate verdicts**: Blocking dimension failures prevent approval regardless of other scores
5. **Mode determines interaction**: Autonomous proceeds without questions; interactive presents dimension menu

## Inputs

| Input | Source | Required | Description |
|-------|--------|----------|-------------|
| `target_description` | User message | Yes | What is being assessed (e.g., "code review skill", "design doc validator") |
| `target_type` | User message or auto-detect | No | Override: `code`, `document`, `api`, `test`, `claim`, `artifact`, `readiness` |
| `mode` | User message | No | `autonomous` (default) or `interactive` |
| `existing_file` | User message | No | Path to skill/command being updated - read for context |

## Phase 1: Detect Target Type

<analysis>
Before generating framework, determine what is being assessed.
If `target_type` provided explicitly, use it.
Otherwise, analyze `target_description` and `existing_file` (if provided).
</analysis>

**Detection patterns (first match wins):**

| Type | Indicators |
|------|------------|
| `readiness` | "production ready", "deploy", "release", "launch", "go/no-go" |
| `claim` | "verify", "factcheck", "claim", "assertion", "accuracy" |
| `test` | "test suite", "coverage", "test quality", ".test.", "_test." |
| `api` | "MCP tool", "endpoint", "REST API", "function signature", "tool docs" |
| `document` | "design doc", "spec", "RFC", "proposal", ".md" |
| `code` | ".ts", ".py", ".js", "review", "audit", "PR", "diff" |
| `artifact` | (default fallback) |

**Output:** Announce detected type: "Target type: [type]"

## Phase 2: Select Dimensions

**Default dimensions by target type:**

| Type | Default Dimensions |
|------|-------------------|
| `code` | correctness, security, error_handling, maintainability |
| `document` | completeness, clarity, accuracy, actionability |
| `api` | documentation, discoverability, error_semantics, examples |
| `test` | coverage, assertion_quality, determinism, edge_cases |
| `claim` | verifiability, accuracy, completeness |
| `artifact` | completeness, correctness, usability |
| `readiness` | functionality, testing, rollback, dependencies |

### Autonomous Mode (default)

Use default dimensions for detected target type. Proceed directly to Phase 3.

### Interactive Mode

Present dimension menu using `mcp_question` tool:

```
mcp_question(questions=[{
    "header": "Assessment Dimensions",
    "question": "Which dimensions should this assessment evaluate?",
    "multiple": True,
    "options": [dimension options for detected type with "(Recommended)" suffix on defaults]
}])
```

**Response handling:**
- Strip " (Recommended)" suffix from selected labels
- Convert to lowercase for dimension IDs
- Minimum 1 dimension required - re-prompt if empty
- Custom answers become custom dimensions (non-blocking by default)

## Phase 3: Generate Framework

Output the following markdown framework, customized for the detected type and selected dimensions:

---

### Framework Output Template

```
## Assessment Framework: [Target Type]

Generated by `/design-assessment` for [target_description]

---

### Dimensions

| Dimension | 0 (Broken) | 3 (Adequate) | 5 (Excellent) | Blocking? |
|-----------|------------|--------------|---------------|-----------|
[Generate row for each selected dimension with type-appropriate descriptions]

---

### Severity Levels

| Level | Priority | Definition | Blocks Approval? |
|-------|----------|------------|------------------|
| CRITICAL | 0 | Must fix immediately - security, data loss, crashes | Yes |
| HIGH | 1 | Must fix before merge - bugs, broken functionality | Yes |
| MEDIUM | 2 | Should fix - code quality, maintainability | No |
| LOW | 3 | Nice to have - minor improvements | No |
| NIT | 4 | Style/preference - optional | No |

---

### Confidence Levels

| Level | Evidence Required | Use When |
|-------|-------------------|----------|
| VERIFIED | Direct evidence (code, test output, docs) | Claim checked against source |
| HIGH | Multiple supporting signals | Strong circumstantial evidence |
| MEDIUM | Context supports but not confirmed | Reasonable inference |
| LOW | Limited or conflicting evidence | Uncertain |
| UNVERIFIED | No supporting evidence | Unable to check |

---

### Finding Schema

\`\`\`yaml
finding:
  id: string          # Unique identifier (e.g., "[PREFIX]-001")
  dimension: string   # Which dimension this relates to
  severity: enum      # CRITICAL | HIGH | MEDIUM | LOW | NIT
  confidence: enum    # VERIFIED | HIGH | MEDIUM | LOW | UNVERIFIED
  location: string    # File:line or section reference
  summary: string     # One-line description
  details: string     # Full explanation
  evidence: string    # What supports this finding
  suggestion: string  # Recommended fix (optional)
  effort: enum        # trivial | moderate | significant
\`\`\`

---

### Verdict Logic

| Condition | Verdict | Action |
|-----------|---------|--------|
[Generate type-appropriate verdict table]

---

### Scorecard Template

| Dimension | Score (0-5) | Justification | Findings |
|-----------|-------------|---------------|----------|
[Generate row for each selected dimension]
| **Overall** | [weighted avg] | | |

**Scoring Guide:**
- 0: Broken - does not function
- 1: Poor - major issues
- 2: Below adequate - significant gaps
- 3: Adequate - meets minimum bar
- 4: Good - above expectations
- 5: Excellent - exemplary

---

### Quality Gate Checklist

- [ ] All blocking dimensions score >= 3
- [ ] No CRITICAL or HIGH severity findings
- [ ] All findings have actionable suggestions
- [ ] Evidence provided for each finding
- [ ] Overall score meets threshold (default: 3.0)
```

---

## Dimension Definitions by Type

### Code Dimensions

| Dimension | 0 (Broken) | 3 (Adequate) | 5 (Excellent) | Blocking? |
|-----------|------------|--------------|---------------|-----------|
| correctness | Code crashes or produces wrong results | Works for happy path | Handles all cases correctly | Yes |
| security | Exploitable vulnerabilities present | No obvious vulnerabilities | Defense in depth, follows best practices | Yes |
| error_handling | Errors swallowed or crash app | Errors caught and logged | Graceful degradation, actionable messages | Yes |
| maintainability | Unreadable, no structure | Readable, basic structure | Self-documenting, well-organized | No |
| performance | Unusable performance | Acceptable performance | Optimized for use case | Conditional |
| testing | No tests or failing tests | Happy path tested | Comprehensive coverage | Conditional |
| style | Inconsistent, violates conventions | Follows conventions | Exemplary style | No |

### Document Dimensions

| Dimension | 0 (Broken) | 3 (Adequate) | 5 (Excellent) | Blocking? |
|-----------|------------|--------------|---------------|-----------|
| completeness | Missing critical sections | All required sections present | Comprehensive, anticipates questions | Yes |
| clarity | Ambiguous, contradictory | Understandable with effort | Crystal clear, no ambiguity | Yes |
| accuracy | Technical errors present | Technically sound | Verified against code/docs | Yes |
| actionability | Cannot implement from this | Can implement with questions | Can implement directly | Conditional |
| consistency | Conflicts with other docs | Aligns with other docs | Exemplary consistency | No |
| scope | Inappropriate scope | Appropriate scope | Perfectly scoped | No |

### API/Tool Dimensions

| Dimension | 0 (Broken) | 3 (Adequate) | 5 (Excellent) | Blocking? |
|-----------|------------|--------------|---------------|-----------|
| documentation | Undocumented | Params and returns documented | Complete with examples and edge cases | Yes |
| discoverability | LLM cannot find or understand | LLM can use with effort | LLM uses naturally | Yes |
| error_semantics | Unclear error conditions | Errors documented | Errors actionable with recovery guidance | Yes |
| idempotency | Unpredictable on retry | Documented retry behavior | Safely idempotent | Conditional |
| naming | Confusing names | Adequate names | Self-explanatory names | No |
| examples | No examples | Basic examples | Comprehensive examples | No |

### Test Dimensions

| Dimension | 0 (Broken) | 3 (Adequate) | 5 (Excellent) | Blocking? |
|-----------|------------|--------------|---------------|-----------|
| coverage | Critical paths untested | Happy paths tested | Comprehensive coverage | Yes |
| assertion_quality | No meaningful assertions | Basic assertions | Precise, behavior-verifying assertions | Yes |
| isolation | Tests interfere with each other | Tests mostly independent | Fully isolated tests | Conditional |
| determinism | Flaky tests | Usually deterministic | Always deterministic | Yes |
| readability | Cannot understand what's tested | Understandable tests | Self-documenting tests | No |
| edge_cases | No edge case testing | Some edge cases | Comprehensive edge cases | Conditional |

### Claim Dimensions

| Dimension | 0 (Broken) | 3 (Adequate) | 5 (Excellent) | Blocking? |
|-----------|------------|--------------|---------------|-----------|
| verifiability | Cannot be checked | Can be checked with effort | Easily verifiable | Yes |
| accuracy | Claim is false | Claim is true | Claim is verified with citation | Yes |
| completeness | Missing critical context | Adequate context | Complete context | Conditional |
| currency | Information is outdated | Information is current | Information is verified current | Conditional |
| relevance | Claim is unnecessary | Claim is relevant | Claim is essential | No |

### Artifact Dimensions

| Dimension | 0 (Broken) | 3 (Adequate) | 5 (Excellent) | Blocking? |
|-----------|------------|--------------|---------------|-----------|
| completeness | Missing expected outputs | All outputs present | Comprehensive outputs | Yes |
| correctness | Content is wrong | Content is accurate | Content is verified | Yes |
| format | Wrong structure | Correct structure | Exemplary structure | Conditional |
| usability | Target audience cannot use | Usable with effort | Immediately usable | Conditional |

### Readiness Dimensions

| Dimension | 0 (Broken) | 3 (Adequate) | 5 (Excellent) | Blocking? |
|-----------|------------|--------------|---------------|-----------|
| functionality | Core features broken | All features work | Edge cases handled | Yes |
| testing | No tests or failing tests | Happy path tested | Comprehensive coverage | Yes |
| documentation | No docs | Basic user docs | Complete user + dev docs | Conditional |
| observability | No monitoring | Basic logging | Full metrics, alerts, dashboards | Conditional |
| rollback | No rollback possible | Manual rollback documented | Automated rollback tested | Yes |
| dependencies | Unstable/unavailable deps | Deps stable and versioned | Deps monitored, fallbacks exist | Yes |

## Verdict Tables by Type

### Code Verdicts

| Condition | Verdict | Action |
|-----------|---------|--------|
| Any CRITICAL | REQUEST_CHANGES | Block merge, fix immediately |
| Any HIGH | REQUEST_CHANGES | Block merge, must fix |
| Only MEDIUM/LOW/NIT | APPROVE | Can merge, consider feedback |
| No findings | APPROVE | Ready to merge |

### Document Verdicts

| Condition | Verdict | Action |
|-----------|---------|--------|
| Any MISSING blocking section | NOT_READY | Cannot proceed to implementation |
| Any VAGUE blocking section | NEEDS_WORK | Clarify before implementation |
| All blocking sections SPECIFIED | READY | Can proceed to implementation planning |

### Readiness Verdicts

| Condition | Verdict | Action |
|-----------|---------|--------|
| Any blocking dimension < 3 | NO_GO | Cannot deploy |
| All blocking dimensions >= 3 | GO | Ready to deploy |
| Conditional dimensions < 3 | GO_WITH_CAVEATS | Deploy with monitoring |

### Default Verdicts (api, test, claim, artifact)

| Condition | Verdict | Action |
|-----------|---------|--------|
| Any CRITICAL findings | REJECT | Must fix before proceeding |
| Any HIGH findings | CHANGES_REQUESTED | Must fix before approval |
| Only MEDIUM or lower | APPROVE_WITH_COMMENTS | Can proceed, should address |
| No findings | APPROVE | Ready to proceed |

## Output

Display the complete framework markdown for the user to copy relevant sections into their skill or command.

<FORBIDDEN>
- Generating framework without detecting target type first
- Skipping dimension selection in interactive mode
- Using severity levels other than CRITICAL/HIGH/MEDIUM/LOW/NIT
- Using confidence levels other than VERIFIED/HIGH/MEDIUM/LOW/UNVERIFIED
- Omitting the finding schema from output
- Generating empty dimension tables
- Proceeding with zero dimensions selected
</FORBIDDEN>

<reflection>
After generating framework:
- Did I detect the correct target type?
- Are all selected dimensions included in the output?
- Is the verdict table appropriate for this target type?
- Is the finding schema complete with all fields?
- Can the user copy-paste this directly into their skill/command?
</reflection>
``````````
